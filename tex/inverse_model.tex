%#!pdflatex Naruse_Esurf_2020.tex

\section{Inverse modeling by deep learning NN}
In this study, numerical simulation of a turbidity current is repeated under various random initial conditions to produce a data set of the characteristic features of turbidites. Then, this artificial dataset of turbidites is used for supervised training of a deep learning NN. The values of the turbidites characteristics, i.e., distribution of volume-per-unit-area of all grain size classes, in the training data set are input to the NN, and the estimated initial conditions (e.g., initial flow height and concentration) of the turbidity current is obtained from the output nodes of NN. The output values of the NN are compared with the true conditions. The optimization of weight coefficients of NN is then conducted to reduce the mean square of the difference between the true conditions and the output values of the NN. If the number of training datasets is sufficiently large, the trained NN should be able to estimate the paleo-hydraulic conditions from the data of the ancient turbidites (Fig. \ref{fig:schematic_diagram_procedures}). In other words, an empirical relationship with numerical results and the model input parameters are explored in this method, and the discovered relationship is used for inverse modeling of turbidity currents. The details of these procedures are described below.

\subsection{Production and preprocessing of training and test data sets for supervised machine learning}

We conducted iterative calculations using the forward model and accumulated data to train and validate the inverse model. To investigate the appropriate amounts of data for training the inverse model, we conducted 500--3500 iteration of the forward model calculations. To verify the performance of the trained model, 300 test data sets were also generated numerically, independent of the training data.

Model input parameters that are subject to inversion are required to produce the training and test data by the forward model calculation (Fig. \ref{fig:model_input_parameters}). In this study, the model inputs are the initial flow height $H_0$, the initial flow length $l_0$, the initial sediment concentration for the $i$th grain size class $C_i$, and the basin slope $S$. These model parameters are generated as uniform random numbers within a certain range, and their range is changed according to the target of the inverse analysis. Since this study is aimed at field-scale analysis, the following ranges are chosen. Both initial depth and length of suspended cloud range from 50 to 600 m. The sediment concentration for each grain size class ranges from 0.01\% to 1.0\%. The number of grain size classes $N$ is four, and the representative grain diameters are 1.5, 2.5, 3.5 and 4.5 phi. The inclination of the basin plain where the turbidites are expected to form ranges from 0 to 1.0\%. 

Each run of the forward model calculation is initiated with the given model input parameters, and is terminated when the flow head reaches the downstream end or sufficiently long time period ($1.2 \times 10^5$ s.) has elapsed. As a result of the calculation, the forward model outputs the volume-per-unit-area of sediment for all grain size classes over the 100 km-long calculation domain. The inverse model estimates the model input parameters from the resultant spatial distribution of the granulometric characteristics of the deposits. However, in natural outcrops, it is unlikely that the entire distribution of the turbidite beds would be exposed. Therefore, we limit the length of the sampling window in the calculation domain, and only the sediment data contained in this window is extracted for both training and testing. The upstream end of the sampling window was set at the transition point between the steep slope and the basin plain (5 km from the upstream end), and the length of the window varies from 1 to 30 km to evaluate the data interval required for the inverse analysis.

Before the model input parameters are input to NN, all values are normalized between 0 and 1 using the following equation:

\begin{equation}
I_{i}^{*} = \frac{I_i - I_{min}}{I_{max} - I_{min}}
\label{eq:normalization}
\end{equation}

\noindent where $I_i^*$ and $I_i$ denote the $i$th normalized and original input parameters, respectively. $I_{\mathrm{max}i}$ and $I_{\mathrm{min}i}$ are the maximum and minimum values used for generating the $i$th input parameter, respectively. This min-max normalization is applied to consider all parameters at equal weights because the range of the initial flow conditions is significantly different between them.

\subsection{Structure of NN}
The artificial NN is used as the inverse model to reconstruct flow conditions from the depositional architecture. We input the spatial distribution of volume-per-unit-area of multiple grain size classes of a turbidite in the NN, which outputs the values of the flow initial conditions and the basin slope. In this study, we use a fully connected NN that has four hidden layers. The volume-per-unit-area of $N$ grain-size classes of sediment deposited on $M$ spatial grids in the sampling window is given to the input nodes of the NN. Thus, the total number of the NN input nodes is $N \times M$. The number of nodes in all hidden layers is set to 2000 in this study. 

The Rectified Linear Unit (ReLU) activation function is adopted for all NN layers \citep{Nair2010, Glorot2011}. The ReLU is the half-wave rectifier $f(z) = \max(z, 0)$. Compared with other smoother non-linearities, such as $\tanh(z)$ or $1/(1+\exp(-z))$, the ReLU typically learns much faster in NN with multiple layers \citep{Glorot2011}, and thus it allows to train a deep supervised network without unsupervised pre-training \citep{LeCun2015}.

The NN is expected to output the model input parameters (i.e., the initial flow conditions and the basin slope), and therefore, the number of nodes in the output layer is equal to the number of input parameters for the forward model, which is seven here (the initial flow length, depth, sediment concentrations and the basin slope).

\subsection{Training the inverse model}
To develop the inverse model, supervised training is conducted using the artificial dataset produced by the forward model calculation. First, the artificial dataset is randomly split into training and validation datasets to detect overfitting during the training process. The ratio of the validation dataset is set to 0.2 so that 80\% of the artificial dataset is used for training. The model input parameters used for producing training and validation sets were regarded as the teacher data to train and evaluate the model.

Methodology applied for training the NN is as follows. The mean squared error (MSE) is adopted as the loss function because the supervised training of NN in this study is classified as a regression problem \citep{Specht1991}, and MSE is a common loss function for regression \citep{Bishop2006,Hastie2009,ShalevShwartz2014}. Before training, all weight coefficients of NN are randomly initialized using the Glorot uniform distribution \citep{Glorot2010}. The backpropagation algorithm \citep{Rumelhart1986} is used to calculate the derivative of this error metric for each connection between the nodes, and the stochastic gradient descent method (SGD) with Nesterov momentum \citep{NESTEROV1983} is used for optimizing the weight coefficients of NN to minimize the difference between the model predictions and the teacher datasets. Other optimization methods, such as AdaGrad \citep{Duchi2011}, RMSprop \citep{Tieleman2012} and AdaDelta \citep{Zeiler2012}, have been tested, but SGD shows the best performance in this case. Dropout regularization \citep{Srivastava2014} is applied for each epoch to reduce overfitting and to improve the generalization ability of the NN. One training epoch, which refers to one cycle through the full training dataset, is repeated until the loss function of the validation dataset converges to a constant value. These methods are all implemented in Python with the library Tensorflow 2.1.0 \citep{Raschka2019}, and the calculations are conducted using GPU NVIDIA GeForce GTX 2080 Super with libraries CUDA 11.0 and CuDNN 7.0.

Several hyperparameters should be specified for the training of NN. Specifically, the dropout rate, the learning rate, the batch size, the number of epochs, and the momentum are adjusted manually after repeated trial and error. To perform an optimization calculation with SGD, the batch size and the learning rate were set to 32 and 0.02, and the value 0.9 was chosen for the momentum. Dropout rate for regularization was 0.5. 

\subsection{Testing the inverse model}

The performance of the inverse model is tested using a set of 300 data that are produced independently of the training and validation datasets. The inversion precision for each model input parameter is evaluated by the root mean square error (RMSE) and the mean absolute error (MAE) of the prediction. These error metrics are computed for both raw and normalized values with true values, and used to evaluate the model. Moreover, the bias of prediction (i.e., the mean deviation of the model predictions from the true input parameters) is used describe the accuracy of the inversion. 

Two additional tests are conducted for verifying the robustness of the inverse model that is significant for the applicability of the model to field datasets. The results of these tests are evaluated by the average of the normalized RMSE, which is defined as:

\begin{equation}
  \mathrm{RMSE} = \sqrt{ \frac{1}{JK} \sum_J \sum_K {\left( \frac{I_{\mathrm{p}jk} - I_{jk}}{I_{jk}} \right)^2} }
  \label{eq:RMSE}
\end{equation}

\noindent where $I_{\mathrm{p}jk}$ and $I_{jk}$ denote the predicted and the original values of the $j$th model input parameter for the $k$th test dataset, respectively. $J$ and $K$ are the numbers of the model input parameters and the test data sets.

First, noise is artificially added to the test data to evaluate the robustness of the inversion results against the measurement error. Under natural conditions, measurement errors in the thickness and grain size analysis of turbidites as well as the local topography  affect these results. If the results of the inverse analysis change significantly due to such errors, it means that our method is not suitable for application to field data. To investigate this, we apply normal random numbers to the volume per unit area at each grid point in the training data at various rates, and we observe how much influence the noise has on the inverse analysis results.

The second test on the inverse model is to perform a subsampling of the grid points in the training data. Outcrops are not continuous over tens of kilometers, so that the thickness and the grain size distribution of a turbidite in the interval between outcrops can only be obtained by interpolation. To simulate this situation, the grid points in test datasets are randomly removed in this test, and the volume-per-unit-area at the removed grid points is linearly interpolated. By varying the rate at which grid points are removed, this test also allows us to estimate the average interval of the outcrops that are necessary for conducting the inverse analysis. That is, if 90\% of the grid points set at 5 m intervals are removed, and the inverse analysis is conducted on the remaining 10\%, the average distance between the grid points is 50 m. Estimating the outcrop spacing requires obtaining reasonable results of inverse analysis before applying it to the actual field.